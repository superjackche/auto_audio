"""
æ•™è‚²è¯­ä¹‰ç›‘æ§åˆ†æå™¨ - ä¸“é—¨ç”¨äºè¯¾å ‚æ•™å­¦å†…å®¹åˆ†æ
é’ˆå¯¹ä¸­å¤–åˆä½œåŠå­¦æ„è¯†å½¢æ€é£é™©é˜²æ§è®¾è®¡
"""

import re
import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter, defaultdict
import json
from datetime import datetime

logger = logging.getLogger(__name__)

class EducationalSemanticAnalyzer:
    """æ•™è‚²è¯­ä¹‰ç›‘æ§åˆ†æå™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = config or {}
        self._load_educational_keywords()
        self._load_risk_patterns()
        self._initialize_scoring_system()
        
        # å†å²åˆ†æè®°å½•ï¼ˆç”¨äºå‰åé€»è¾‘æ¯”è¾ƒï¼‰
        self.analysis_history = []
        self.max_history = 50  # ä¿ç•™æœ€è¿‘50æ¬¡åˆ†æ
        
    def _load_educational_keywords(self):
        """åŠ è½½æ•™è‚²ç›¸å…³å…³é”®è¯åº“"""
        # æ­£é¢æ•™è‚²å…³é”®è¯
        self.positive_educational_keywords = {
            # çˆ±å›½ä¸»ä¹‰æ•™è‚²
            'çˆ±å›½', 'æ°‘æ—', 'å›½å®¶', 'ç¥–å›½', 'ä¸­åæ°‘æ—', 'æ°‘æ—å›¢ç»“', 'å›½å®¶ç»Ÿä¸€',
            'ç¤¾ä¼šä¸»ä¹‰æ ¸å¿ƒä»·å€¼è§‚', 'ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰', 'æ–°æ—¶ä»£',
            
            # å­¦æœ¯æ­£é¢è¯æ±‡
            'å­¦æœ¯ç ”ç©¶', 'ç§‘å­¦ç²¾ç¥', 'åˆ›æ–°', 'å®è·µ', 'ç†è®ºè”ç³»å®é™…', 'æ‰¹åˆ¤æ€§æ€ç»´',
            'å­¦æœ¯è¯šä¿¡', 'å­¦æœ¯è§„èŒƒ', 'ç§‘ç ”ä¼¦ç†', 'å­¦æœ¯äº¤æµ',
            
            # æ•™è‚²æ­£é¢è¯æ±‡
            'æ•™è‚²å…¬å¹³', 'å› ææ–½æ•™', 'ç«‹å¾·æ ‘äºº', 'å…¨é¢å‘å±•', 'ç´ è´¨æ•™è‚²',
            'å¸ˆå¾·å¸ˆé£', 'æ•™ä¹¦è‚²äºº', 'ä¸ºäººå¸ˆè¡¨', 'ä¼ é“æˆä¸šè§£æƒ‘',
            
            # å›½é™…äº¤æµæ­£é¢è¯æ±‡
            'æ–‡åŒ–äº¤æµ', 'äº’å­¦äº’é‰´', 'å’Œè€Œä¸åŒ', 'åŒ…å®¹å¼€æ”¾', 'åˆä½œå…±èµ¢',
            'äººç±»å‘½è¿å…±åŒä½“', 'æ–‡æ˜å¯¹è¯', 'æ–‡åŒ–è‡ªä¿¡'
        }
        
        # é£é™©å…³é”®è¯
        self.risk_keywords = {
            # æ„è¯†å½¢æ€é£é™©è¯æ±‡
            'æ„è¯†å½¢æ€': {'level': 'high', 'weight': 10},
            'æ”¿æ²»åˆ¶åº¦': {'level': 'medium', 'weight': 7},
            'åˆ¶åº¦ä¼˜è¶Šæ€§': {'level': 'medium', 'weight': 6},
            'æ°‘ä¸»è‡ªç”±': {'level': 'medium', 'weight': 5},
            'äººæƒ': {'level': 'medium', 'weight': 5},
            
            # å†å²è™šæ— ä¸»ä¹‰é£é™©
            'å†å²è™šæ— ': {'level': 'high', 'weight': 9},
            'æ–‡é©': {'level': 'high', 'weight': 8},
            'å†å²åæ€': {'level': 'medium', 'weight': 6},
            
            # è¥¿æ–¹ä»·å€¼è§‚é£é™©
            'è¥¿æ–¹ä»·å€¼è§‚': {'level': 'high', 'weight': 8},
            'æ™®ä¸–ä»·å€¼': {'level': 'high', 'weight': 8},
            'ä¸ªäººä¸»ä¹‰': {'level': 'medium', 'weight': 5},
            'è‡ªç”±ä¸»ä¹‰': {'level': 'medium', 'weight': 6},
            
            # å®—æ•™æ¸—é€é£é™©
            'å®—æ•™ä¿¡ä»°': {'level': 'medium', 'weight': 6},
            'åŸºç£æ•™': {'level': 'medium', 'weight': 5},
            'ä¼ æ•™': {'level': 'high', 'weight': 8},
            
            # åˆ†è£‚é£é™©è¯æ±‡
            'å°ç‹¬': {'level': 'critical', 'weight': 15},
            'æ¸¯ç‹¬': {'level': 'critical', 'weight': 15},
            'è—ç‹¬': {'level': 'critical', 'weight': 15},
            'æ–°ç–†é—®é¢˜': {'level': 'high', 'weight': 10},
            'ä¸€å›½ä¸¤åˆ¶': {'level': 'medium', 'weight': 6}
        }
        
        # å­¦æœ¯äº‰è®®è¯æ±‡ï¼ˆéœ€è¦ä¸Šä¸‹æ–‡åˆ†æï¼‰
        self.controversial_academic_terms = {
            'å­¦æœ¯è‡ªç”±', 'è¨€è®ºè‡ªç”±', 'æ–°é—»è‡ªç”±', 'æ‰¹åˆ¤ç²¾ç¥', 'ç‹¬ç«‹æ€è€ƒ',
            'è´¨ç–‘æƒå¨', 'å­¦æœ¯ç‹¬ç«‹', 'ä»·å€¼ä¸­ç«‹', 'å®¢è§‚æ€§', 'å¤šå…ƒåŒ–'
        }
        
    def _load_risk_patterns(self):
        """åŠ è½½é£é™©æ¨¡å¼"""
        self.risk_patterns = {
            # æ¯”è¾ƒæ¨¡å¼ï¼ˆæš—ç¤ºåˆ¶åº¦å¯¹æ¯”ï¼‰
            'comparison_patterns': [
                r'(ä¸­å›½|å›½å†…).*?(ä¸å¦‚|è½åäº|æ¯”ä¸ä¸Š).*(è¥¿æ–¹|æ¬§ç¾|å‘è¾¾å›½å®¶)',
                r'(è¥¿æ–¹|æ¬§ç¾|å‘è¾¾å›½å®¶).*?(å…ˆè¿›|ä¼˜è¶Š|é¢†å…ˆ).*(ä¸­å›½|å›½å†…)',
                r'ä¸ºä»€ä¹ˆ.*?(ä¸­å›½|æˆ‘ä»¬).*?ä¸èƒ½.*?(åƒ.*?ä¸€æ ·|å­¦ä¹ .*?)',
            ],
            
            # æš—ç¤ºæ¨¡å¼
            'implication_patterns': [
                r'ä½ ä»¬(åº”è¯¥|éœ€è¦|å¿…é¡»).*?(æ€è€ƒ|åæ€|è´¨ç–‘)',
                r'(çœŸæ­£çš„|çœŸå®çš„).*?(å†å²|äº‹å®|çœŸç›¸)',
                r'(å®˜æ–¹|æ”¿åºœ).*?(è¯´æ³•|ç‰ˆæœ¬|è§£é‡Š).*?(ä½†æ˜¯|ç„¶è€Œ|å®é™…ä¸Š)',
            ],
            
            # ä»·å€¼è§‚å¼•å¯¼æ¨¡å¼
            'value_guidance_patterns': [
                r'(è‡ªç”±|æ°‘ä¸»|äººæƒ).*?(é‡è¦|å®è´µ|çè´µ)',
                r'æ¯ä¸ªäºº.*?(éƒ½æœ‰æƒ|åº”è¯¥).*?(é€‰æ‹©|å†³å®š|è¡¨è¾¾)',
                r'(å¤šå…ƒåŒ–|åŒ…å®¹æ€§).*?(ç¤¾ä¼š|æ–‡åŒ–|è§‚å¿µ)',
            ],
            
            # å†å²å¦å®šæ¨¡å¼
            'history_denial_patterns': [
                r'(å†å²|è¿‡å»).*?(é”™è¯¯|é—®é¢˜|åæ€)',
                r'(é‚£ä¸ªæ—¶ä»£|å½“æ—¶).*?(ä¸äº†è§£|ä¸çŸ¥é“|è¢«è¯¯å¯¼)',
                r'ç°åœ¨.*?(é‡æ–°|å®¢è§‚|ç†æ€§).*?(çœ‹å¾…|è¯„ä»·|è®¤è¯†)',
            ]
        }
    
    def _initialize_scoring_system(self):
        """åˆå§‹åŒ–è¯„åˆ†ç³»ç»Ÿ"""
        self.scoring_weights = {
            'keyword_risk': 0.4,      # å…³é”®è¯é£é™©æƒé‡
            'pattern_risk': 0.3,      # æ¨¡å¼é£é™©æƒé‡
            'logic_consistency': 0.2,  # é€»è¾‘ä¸€è‡´æ€§æƒé‡
            'sentiment_bias': 0.1     # æƒ…æ„Ÿå€¾å‘æƒé‡
        }
        
        # é£é™©ç­‰çº§é˜ˆå€¼
        self.risk_thresholds = {
            'low': 30,
            'medium': 50,
            'high': 70,
            'critical': 85
        }
    
    def analyze_educational_content(self, text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        åˆ†ææ•™è‚²å†…å®¹
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬ï¼ˆè¯¾å ‚è®²è¯å†…å®¹ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆè¯¾ç¨‹ã€æ•™å¸ˆã€æ—¶é—´ç­‰ï¼‰
            
        Returns:
            è¯¦ç»†çš„åˆ†æç»“æœ
        """
        if not text or not text.strip():
            return self._empty_analysis_result()
        
        # åŸºç¡€æ–‡æœ¬åˆ†æ
        basic_analysis = self._basic_text_analysis(text)
        
        # å…³é”®è¯é£é™©åˆ†æ
        keyword_analysis = self._analyze_keywords(text)
        
        # æ¨¡å¼é£é™©åˆ†æ
        pattern_analysis = self._analyze_patterns(text)
        
        # é€»è¾‘ä¸€è‡´æ€§åˆ†æ
        logic_analysis = self._analyze_logic_consistency(text, basic_analysis)
        
        # æƒ…æ„Ÿå€¾å‘åˆ†æ
        sentiment_analysis = self._analyze_educational_sentiment(text)
        
        # ç»¼åˆé£é™©è¯„åˆ†
        risk_score = self._calculate_risk_score(
            keyword_analysis, pattern_analysis, logic_analysis, sentiment_analysis
        )
        
        # ç”Ÿæˆåˆ†æç»“æœ
        analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'text_length': len(text),
            'basic_analysis': basic_analysis,
            'keyword_analysis': keyword_analysis,
            'pattern_analysis': pattern_analysis,
            'logic_analysis': logic_analysis,
            'sentiment_analysis': sentiment_analysis,
            'risk_score': risk_score,
            'risk_level': self._determine_risk_level(risk_score['total_score']),
            'recommendations': self._generate_recommendations(risk_score, keyword_analysis, pattern_analysis),
            'context': context or {}
        }
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self._save_to_history(analysis_result)
        
        return analysis_result
    
    def _basic_text_analysis(self, text: str) -> Dict[str, Any]:
        """åŸºç¡€æ–‡æœ¬åˆ†æ"""
        # ç®€å•åˆ†è¯
        words = self._simple_tokenize(text)
        
        # è¯­è¨€æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = chinese_chars + english_chars
        
        language = 'unknown'
        if total_chars > 0:
            chinese_ratio = chinese_chars / total_chars
            if chinese_ratio > 0.7:
                language = 'chinese'
            elif chinese_ratio < 0.3:
                language = 'english'
            else:
                language = 'mixed'
        
        return {
            'word_count': len(words),
            'char_count': len(text),
            'language': language,
            'tokens': words[:20]  # åªä¿ç•™å‰20ä¸ªè¯æ±‡
        }
    
    def _analyze_keywords(self, text: str) -> Dict[str, Any]:
        """å…³é”®è¯é£é™©åˆ†æ"""
        found_risk_keywords = []
        found_positive_keywords = []
        found_controversial_keywords = []
        
        text_lower = text.lower()
        
        # æ£€æŸ¥é£é™©å…³é”®è¯
        for keyword, info in self.risk_keywords.items():
            if keyword in text:
                found_risk_keywords.append({
                    'keyword': keyword,
                    'level': info['level'],
                    'weight': info['weight'],
                    'context': self._extract_context(text, keyword)
                })
        
        # æ£€æŸ¥æ­£é¢æ•™è‚²å…³é”®è¯
        for keyword in self.positive_educational_keywords:
            if keyword in text:
                found_positive_keywords.append({
                    'keyword': keyword,
                    'context': self._extract_context(text, keyword)
                })
        
        # æ£€æŸ¥äº‰è®®æ€§å­¦æœ¯è¯æ±‡
        for keyword in self.controversial_academic_terms:
            if keyword in text:
                found_controversial_keywords.append({
                    'keyword': keyword,
                    'context': self._extract_context(text, keyword)
                })
        
        # è®¡ç®—å…³é”®è¯é£é™©åˆ†æ•°
        risk_score = sum(kw['weight'] for kw in found_risk_keywords)
        positive_score = len(found_positive_keywords) * 2  # æ­£é¢è¯æ±‡é™ä½é£é™©
        
        return {
            'risk_keywords': found_risk_keywords,
            'positive_keywords': found_positive_keywords,
            'controversial_keywords': found_controversial_keywords,
            'risk_score': max(0, risk_score - positive_score),
            'keyword_density': len(found_risk_keywords) / max(1, len(text.split()))
        }
    
    def _analyze_patterns(self, text: str) -> Dict[str, Any]:
        """æ¨¡å¼é£é™©åˆ†æ"""
        found_patterns = []
        
        for pattern_type, patterns in self.risk_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    found_patterns.append({
                        'type': pattern_type,
                        'pattern': pattern,
                        'matched_text': match.group(),
                        'position': match.span(),
                        'context': self._extract_context(text, match.group())
                    })
        
        # è®¡ç®—æ¨¡å¼é£é™©åˆ†æ•°
        pattern_weights = {
            'comparison_patterns': 8,
            'implication_patterns': 6,
            'value_guidance_patterns': 7,
            'history_denial_patterns': 9
        }
        
        pattern_score = sum(pattern_weights.get(p['type'], 5) for p in found_patterns)
        
        return {
            'found_patterns': found_patterns,
            'pattern_count': len(found_patterns),
            'pattern_score': pattern_score
        }
    
    def _analyze_logic_consistency(self, text: str, basic_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """é€»è¾‘ä¸€è‡´æ€§åˆ†æ"""
        consistency_score = 100  # åŸºç¡€åˆ†æ•°
        inconsistencies = []
        
        # æ£€æŸ¥ä¸å†å²å†…å®¹çš„ä¸€è‡´æ€§
        if len(self.analysis_history) > 0:
            recent_analyses = self.analysis_history[-5:]  # æœ€è¿‘5æ¬¡åˆ†æ
            
            # æ£€æŸ¥ä¸»é¢˜ä¸€è‡´æ€§
            current_keywords = set()
            if basic_analysis.get('tokens'):
                current_keywords = set(basic_analysis['tokens'][:10])
            
            for historical in recent_analyses:
                if 'basic_analysis' in historical and 'tokens' in historical['basic_analysis']:
                    historical_keywords = set(historical['basic_analysis']['tokens'][:10])
                    similarity = len(current_keywords & historical_keywords) / max(1, len(current_keywords | historical_keywords))
                    
                    if similarity < 0.3:  # ä¸»é¢˜å˜åŒ–è¾ƒå¤§
                        inconsistencies.append({
                            'type': 'topic_shift',
                            'description': 'ä¸»é¢˜å˜åŒ–è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨è¯é¢˜è·³è·ƒ',
                            'severity': 'medium'
                        })
                        consistency_score -= 10
        
        # æ£€æŸ¥æ–‡æœ¬å†…éƒ¨é€»è¾‘
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', text)
        if len(sentences) > 2:
            # æ£€æŸ¥è½¬æŠ˜è¯ä½¿ç”¨
            transition_words = ['ä½†æ˜¯', 'ç„¶è€Œ', 'ä¸è¿‡', 'å®é™…ä¸Š', 'äº‹å®ä¸Š', 'å…¶å®']
            transition_count = sum(1 for word in transition_words if word in text)
            
            if transition_count > len(sentences) * 0.3:  # è½¬æŠ˜è¯è¿‡å¤š
                inconsistencies.append({
                    'type': 'excessive_transitions',
                    'description': 'è½¬æŠ˜è¯ä½¿ç”¨è¿‡å¤šï¼Œå¯èƒ½å­˜åœ¨è§‚ç‚¹æ‘‡æ‘†',
                    'severity': 'low'
                })
                consistency_score -= 5
        
        return {
            'consistency_score': max(0, consistency_score),
            'inconsistencies': inconsistencies,
            'analysis_count': len(self.analysis_history)
        }
    
    def _analyze_educational_sentiment(self, text: str) -> Dict[str, Any]:
        """æ•™è‚²æƒ…æ„Ÿåˆ†æ"""
        # æ•™è‚²æ­£é¢æƒ…æ„Ÿè¯æ±‡
        positive_educational_words = {
            'ä¼˜ç§€', 'å“è¶Š', 'è¿›æ­¥', 'å‘å±•', 'æˆå°±', 'è£èª‰', 'è‡ªè±ª', 'éª„å‚²',
            'å¸Œæœ›', 'æœªæ¥', 'æ¢¦æƒ³', 'ç†æƒ³', 'å¥‹æ–—', 'åŠªåŠ›', 'åšæŒ', 'æˆåŠŸ',
            'å­¦ä¹ ', 'æˆé•¿', 'æé«˜', 'æ”¹å–„', 'åˆ›æ–°', 'çªç ´', 'è´¡çŒ®', 'å¥‰çŒ®'
        }
        
        # è´Ÿé¢æˆ–äº‰è®®æƒ…æ„Ÿè¯æ±‡
        negative_educational_words = {
            'è½å', 'å¤±è´¥', 'é”™è¯¯', 'é—®é¢˜', 'å›°éš¾', 'æŒ‘æˆ˜', 'æ‰¹è¯„', 'è´¨ç–‘',
            'åå¯¹', 'æŠ—è®®', 'ä¸æ»¡', 'æ„¤æ€’', 'å¤±æœ›', 'æ‹…å¿ƒ', 'å¿§è™‘', 'ææƒ§',
            'æ··ä¹±', 'å±æœº', 'å†²çª', 'å¯¹ç«‹', 'åˆ†æ­§', 'äº‰è®®', 'æ€€ç–‘', 'å¦å®š'
        }
        
        words = self._simple_tokenize(text)
        
        positive_count = sum(1 for word in words if word in positive_educational_words)
        negative_count = sum(1 for word in words if word in negative_educational_words)
        
        total_emotional_words = positive_count + negative_count
        
        if total_emotional_words == 0:
            sentiment = 'neutral'
            bias_score = 0
        else:
            positive_ratio = positive_count / total_emotional_words
            if positive_ratio > 0.6:
                sentiment = 'positive'
                bias_score = 0
            elif positive_ratio < 0.4:
                sentiment = 'negative'
                bias_score = (0.4 - positive_ratio) * 50  # è´Ÿé¢æƒ…æ„Ÿçš„é£é™©åˆ†æ•°
            else:
                sentiment = 'neutral'
                bias_score = 0
        
        return {
            'sentiment': sentiment,
            'positive_count': positive_count,
            'negative_count': negative_count,
            'bias_score': bias_score,
            'emotional_intensity': total_emotional_words / max(1, len(words))
        }
    
    def _calculate_risk_score(self, keyword_analysis: Dict, pattern_analysis: Dict, 
                            logic_analysis: Dict, sentiment_analysis: Dict) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆé£é™©è¯„åˆ†"""
        
        # å„é¡¹åˆ†æ•°
        keyword_score = keyword_analysis.get('risk_score', 0)
        pattern_score = pattern_analysis.get('pattern_score', 0)
        logic_score = max(0, 100 - logic_analysis.get('consistency_score', 100))
        sentiment_score = sentiment_analysis.get('bias_score', 0)
        
        # åŠ æƒè®¡ç®—
        weights = self.scoring_weights
        total_score = (
            keyword_score * weights['keyword_risk'] +
            pattern_score * weights['pattern_risk'] +
            logic_score * weights['logic_consistency'] +
            sentiment_score * weights['sentiment_bias']
        )
        
        return {
            'keyword_score': keyword_score,
            'pattern_score': pattern_score,
            'logic_score': logic_score,
            'sentiment_score': sentiment_score,
            'total_score': min(100, total_score),  # æœ€é«˜100åˆ†
            'score_breakdown': {
                'keyword_risk': keyword_score * weights['keyword_risk'],
                'pattern_risk': pattern_score * weights['pattern_risk'],
                'logic_consistency': logic_score * weights['logic_consistency'],
                'sentiment_bias': sentiment_score * weights['sentiment_bias']
            }
        }
    
    def _determine_risk_level(self, score: float) -> str:
        """ç¡®å®šé£é™©ç­‰çº§"""
        if score >= self.risk_thresholds['critical']:
            return 'critical'
        elif score >= self.risk_thresholds['high']:
            return 'high'
        elif score >= self.risk_thresholds['medium']:
            return 'medium'
        else:
            return 'low'
    
    def _generate_recommendations(self, risk_score: Dict, keyword_analysis: Dict, 
                                pattern_analysis: Dict) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        total_score = risk_score['total_score']
        
        if total_score >= self.risk_thresholds['high']:
            recommendations.append("ğŸš¨ å»ºè®®ç«‹å³å…³æ³¨ï¼šæ£€æµ‹åˆ°é«˜é£é™©å†…å®¹ï¼Œéœ€è¦åŠæ—¶å¹²é¢„")
        elif total_score >= self.risk_thresholds['medium']:
            recommendations.append("âš ï¸ å»ºè®®å…³æ³¨ï¼šæ£€æµ‹åˆ°ä¸­ç­‰é£é™©å†…å®¹ï¼Œå»ºè®®è¿›ä¸€æ­¥è§‚å¯Ÿ")
        
        # å…·ä½“å»ºè®®
        if keyword_analysis.get('risk_keywords'):
            recommendations.append("â€¢ æ³¨æ„æ•æ„Ÿå…³é”®è¯çš„ä½¿ç”¨ï¼Œå»ºè®®è°¨æ…è¡¨è¾¾ç›¸å…³å†…å®¹")
        
        if pattern_analysis.get('found_patterns'):
            recommendations.append("â€¢ æ£€æµ‹åˆ°æ½œåœ¨é£é™©è¡¨è¾¾æ¨¡å¼ï¼Œå»ºè®®è°ƒæ•´è¡¨è¾¾æ–¹å¼")
        
        if len(keyword_analysis.get('positive_keywords', [])) < 2:
            recommendations.append("â€¢ å»ºè®®å¢åŠ æ­£é¢æ•™è‚²å¼•å¯¼å†…å®¹")
        
        if not recommendations:
            recommendations.append("âœ… å†…å®¹è¡¨è¾¾è§„èŒƒï¼Œç»§ç»­ä¿æŒ")
        
        return recommendations
    
    def _extract_context(self, text: str, keyword: str, context_length: int = 30) -> str:
        """æå–å…³é”®è¯ä¸Šä¸‹æ–‡"""
        index = text.find(keyword)
        if index == -1:
            return ""
        
        start = max(0, index - context_length)
        end = min(len(text), index + len(keyword) + context_length)
        
        context = text[start:end]
        if start > 0:
            context = "..." + context
        if end < len(text):
            context = context + "..."
            
        return context
    
    def _simple_tokenize(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯"""
        # ä¸­è‹±æ–‡æ··åˆåˆ†è¯
        pattern = r'[\u4e00-\u9fff]+|[a-zA-Z]+|[0-9]+'
        tokens = re.findall(pattern, text)
        return [token.lower() for token in tokens if len(token) > 1]
    
    def _save_to_history(self, analysis_result: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœåˆ°å†å²è®°å½•"""
        self.analysis_history.append(analysis_result)
        
        # ä¿æŒå†å²è®°å½•åœ¨é™åˆ¶èŒƒå›´å†…
        if len(self.analysis_history) > self.max_history:
            self.analysis_history = self.analysis_history[-self.max_history:]
    
    def _empty_analysis_result(self) -> Dict[str, Any]:
        """ç©ºåˆ†æç»“æœ"""
        return {
            'timestamp': datetime.now().isoformat(),
            'text_length': 0,
            'basic_analysis': {'word_count': 0, 'char_count': 0, 'language': 'unknown', 'tokens': []},
            'keyword_analysis': {'risk_keywords': [], 'positive_keywords': [], 'risk_score': 0},
            'pattern_analysis': {'found_patterns': [], 'pattern_score': 0},
            'logic_analysis': {'consistency_score': 100, 'inconsistencies': []},
            'sentiment_analysis': {'sentiment': 'neutral', 'bias_score': 0},
            'risk_score': {'total_score': 0},
            'risk_level': 'low',
            'recommendations': [],
            'context': {}
        }
    
    def get_analysis_summary(self, time_range_hours: int = 24) -> Dict[str, Any]:
        """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„åˆ†ææ‘˜è¦"""
        from datetime import datetime, timedelta
        
        cutoff_time = datetime.now() - timedelta(hours=time_range_hours)
        
        recent_analyses = [
            analysis for analysis in self.analysis_history
            if datetime.fromisoformat(analysis['timestamp']) > cutoff_time
        ]
        
        if not recent_analyses:
            return {'message': 'æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— åˆ†æè®°å½•'}
        
        # ç»Ÿè®¡åˆ†æ
        risk_levels = [analysis['risk_level'] for analysis in recent_analyses]
        risk_level_counts = Counter(risk_levels)
        
        avg_risk_score = sum(analysis['risk_score']['total_score'] for analysis in recent_analyses) / len(recent_analyses)
        
        # é¢‘ç¹å‡ºç°çš„é£é™©å…³é”®è¯
        all_risk_keywords = []
        for analysis in recent_analyses:
            all_risk_keywords.extend([kw['keyword'] for kw in analysis['keyword_analysis']['risk_keywords']])
        
        frequent_risk_keywords = Counter(all_risk_keywords).most_common(5)
        
        return {
            'time_range_hours': time_range_hours,
            'total_analyses': len(recent_analyses),
            'risk_level_distribution': dict(risk_level_counts),
            'average_risk_score': round(avg_risk_score, 2),
            'frequent_risk_keywords': frequent_risk_keywords,
            'high_risk_count': risk_level_counts.get('high', 0) + risk_level_counts.get('critical', 0)
        }
